{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNghoSgj/aWGWaDs9YmpMIM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/blooming-ai/generativeai/blob/main/pytorch_basics/linear_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Classification\n",
        "## Implementation using PyTorch [[1](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html), [2](https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html), [3](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html)]\n",
        "\n",
        "* Dataset\n",
        "* Linear Classification Model\n",
        "* Loss Function\n",
        "* Training\n",
        "* Evaluation"
      ],
      "metadata": {
        "id": "ACuMhy9hFHvG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn #has all neural network modules\n",
        "from torch.nn import functional as F # has many functions required for building a neural network\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "hkEeBsCxFT4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Data"
      ],
      "metadata": {
        "id": "4WngCArCJ1Kb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate data\n",
        "N = 100\n",
        "ones = torch.ones((N,1), dtype = torch.int64)\n",
        "class1  = torch.randn((N,2))# (x,y) stacked one below the other\n",
        "label1 = ones\n",
        "class2  = torch.randn((N,2)) + torch.tensor([[0,10]]) # addition is broadcasted\n",
        "label2 = ones*2\n",
        "class3  = torch.randn((N,2)) + torch.tensor([[10,0]]) # addition is broadcasted\n",
        "label3 = ones*3\n",
        "X = torch.cat( (class1, class2, class3), dim = 0) # concatinate data\n",
        "Y = torch.cat( (label1, label2, label3), dim = 0) # concatinate labels\n",
        "\n",
        "plt.plot(class1[:,0].numpy(), class1[:,1].numpy(), 'bo', label='class1')\n",
        "plt.plot(class2[:,0].numpy(), class2[:,1].numpy(), 'rx', label='class2')\n",
        "plt.plot(class3[:,0].numpy(), class3[:,1].numpy(), 'g+', label='class3')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.legend()\n",
        "plt.grid('True', color='y')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-hHX2G9HGnKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define a Dataset and a Dataloader"
      ],
      "metadata": {
        "id": "HV0O2tXHYw7M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "class Dataset2D(Dataset):\n",
        "    def __init__(self, X:torch.tensor, class_labels:torch.tensor):\n",
        "        self.input = X\n",
        "        self.class_labels = class_labels\n",
        "        self.n_labels = torch.max(class_labels).item() # assumes labels are 1,2,3 ..., .item() extracts data from a tensor\n",
        "        #Create a one hot vector\n",
        "        self.output = torch.zeros((X.size(0), self.n_labels))\n",
        "        label_idx = class_labels - 1\n",
        "        self.output.scatter_(1, label_idx.view(-1,1), 1)\n",
        "\n",
        "        assert(self.input.shape[0] == self.output.shape[0]) # check if every input has a corresponding output\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.input.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input[idx,:], self.output[idx,:]\n",
        "\n",
        "dataset = Dataset2D(X,Y)\n",
        "# Randomly split a dataset into non-overlapping new datasets of given lengths. Fractions summing up to 1 can be given.\n",
        "#The lengths will be floor(frac * len(dataset)) for each fraction provided.\n",
        "training, testing = random_split(dataset, [0.8, 0.2])\n",
        "\n",
        "\n",
        "#Test\n",
        "input, output = dataset[0]\n",
        "print(\"Sample input: \",input, \", output: \", output)\n",
        "print(\"len of training: \", len(training), \"len of testing: \", len(testing) )\n",
        "\n"
      ],
      "metadata": {
        "id": "OzOY3hueY3bi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define model\n",
        "### Linear classifier\n",
        "$y = \\mathbf{W} x + \\mathbf{b} \\qquad\n",
        "x, b \\in \\mathbb{R}^2 \\;\\; y \\in \\mathbb{R}^3 \\;\\; W \\in \\mathbb{R}^{2 \\times 3}$\n",
        "\n",
        "$\\text{Soft Max: } z_i = \\sigma(y_i)= e^y_i / \\sum e^{y_i}$"
      ],
      "metadata": {
        "id": "oPFADCLDJ8Y9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pdb\n",
        "# nn.Module is Base class for all neural network modules. Your models should also subclass this class.\n",
        "# Modules can also contain other Modules, allowing to nest them in a tree structure.\n",
        "class Linear_Classifier(nn.Module):\n",
        "    def __init__(self, feature_dim:int = 2, n_labels:int = 3):\n",
        "        super(Linear_Classifier, self).__init__()\n",
        "        self.feature_dim = feature_dim\n",
        "        self.n_labels = n_labels\n",
        "        self.linear = nn.Linear(self.feature_dim, n_labels ) #(input dim, output dim)\n",
        "\n",
        "        #https://www.geeksforgeeks.org/initialize-weights-in-pytorch/\n",
        "        torch.nn.init.xavier_uniform_(self.linear.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.linear(x)\n",
        "        # pdb.set_trace()\n",
        "        y = F.softmax(y, dim = 1)\n",
        "        return y\n",
        "\n",
        "    @torch.no_grad() # - disables gradient calculation. Useful for inference.\n",
        "    def evaluate(self, input ):\n",
        "        self.eval() # Sets the module in evaluation mode.This affects only on certain modules. e.g. Dropout, BatchNorm, etc.\n",
        "        output_estimate = self(input)\n",
        "        return output_estimate\n",
        "\n"
      ],
      "metadata": {
        "id": "ErD6hklkKAnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss Function\n",
        "$l(Z, \\hat{Z}) = \\frac{1}{N}\\sum_i (z_i - \\hat{z_i})^2$"
      ],
      "metadata": {
        "id": "Y7ry61EIrc7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mean Squared Error (MSE) as our loss function.\n",
        "loss_fun = torch.nn.MSELoss()"
      ],
      "metadata": {
        "id": "XE6Qw7c7rf2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "ZWugQElUW0AK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, loss_fun, optimizer, data_loader, epochs:int = 10):\n",
        "    '''\n",
        "    Preform training using the given optimizer to reduce the loss_fun. The data_loader loads the data.\n",
        "    The training is performed for the number of epochs\n",
        "    '''\n",
        "    model.train() # Set to train mode\n",
        "    history = []\n",
        "    for epoch in range(epochs):\n",
        "        # print(\"===================== epoch: \", epoch,\"=======================\")\n",
        "        train_losses = []\n",
        "        for batch in data_loader:\n",
        "            input, output = batch\n",
        "            output_estimate = model(input) # calls the forward function\n",
        "\n",
        "            loss = loss_fun(output_estimate, output)\n",
        "\n",
        "            train_losses.append(loss.data.item())\n",
        "            #Calculate gradient\n",
        "            loss.backward()\n",
        "            #Take a step in the negative of gradient direction\n",
        "            optimizer.step()\n",
        "            #explicitly set the gradients to zero. Otherwise gradient accumulate.\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        history.extend(train_losses)\n",
        "\n",
        "    return history\n"
      ],
      "metadata": {
        "id": "g06kXpcvW20g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 20 # The number of data samples considered for gradient computation\n",
        "learning_rate = 1e-3 # Gradient descent step size\n",
        "epochs = 200 # the number times to iterate over the dataset\n",
        "\n",
        "#Instantiate the classifier model\n",
        "model = Linear_Classifier()\n",
        "\n",
        "# Reads the dataset into minibatchs. The batches are reshuffled at every epochs\n",
        "train_dataloader = DataLoader(training, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "#Define a gradient descent classifier\n",
        "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
        "\n",
        "#Train the classifier\n",
        "losses = train(model, loss_fun, optimizer, train_dataloader, epochs)\n",
        "plt.plot(losses)\n",
        "plt.show()\n",
        "\n",
        "#Save the model\n",
        "torch.save(model, \"./linear_classifier.pt\")"
      ],
      "metadata": {
        "id": "7ysmb9JesRq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation"
      ],
      "metadata": {
        "id": "vPt4BJDbsKMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.load(\"./linear_classifier.pt\")\n",
        "misclassified = 0\n",
        "for test_data, test_output in testing:\n",
        "    output = model.evaluate(test_data.unsqueeze(dim = 0)).squeeze()\n",
        "    idx_out = torch.argmax(output).item()\n",
        "    idx_expected = torch.argmax(test_output).item()\n",
        "    if idx_out != idx_expected: misclassified += 1\n",
        "\n",
        "print(\"Accuracy: \", round(1- misclassified/len(testing),2))"
      ],
      "metadata": {
        "id": "xvKnlQPWsNbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### H.W Draw - Classifier Lines"
      ],
      "metadata": {
        "id": "NoFYkV4l2FD9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(name, param.data)"
      ],
      "metadata": {
        "id": "FlD3eMoz1ewD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### References for other important topics:\n",
        "* [Setting different learning rates for different layers](https://stackoverflow.com/questions/73629330/what-exactly-is-meant-by-param-groups-in-pytorch)\n",
        "* Managing tensor on CPU or GPU [[1](https://pytorch.org/docs/stable/notes/cuda.html), [2](https://pytorch.org/docs/stable/generated/torch.Tensor.to.html#torch.Tensor.to)]\n",
        "* [Automatic computation of gradient](https://pytorch.org/blog/overview-of-pytorch-autograd-engine/#:~:text=PyTorch%20computes%20the%20gradient%20of,ways%3B%20forward%20and%20reverse%20mode.)\n",
        "* [Loss function](https://pytorch.org/docs/stable/nn.functional.html#loss-functions)\n",
        "* [Non-linear activation functions](https://pytorch.org/docs/stable/nn.functional.html#non-linear-activation-functions)\n",
        "* [Normalization techniques](https://towardsdatascience.com/different-normalization-layers-in-deep-learning-1a7214ff71d6)"
      ],
      "metadata": {
        "id": "wJ_6uExWmGBD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m6xCzFVp5R8w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}