{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMuruWUA7II9QtRrLRpf8PQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/blooming-ai/generativeai/blob/main/text/byte_pair_encoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Language Models an Introduction\n",
        "\n",
        "\n",
        "Most of the code snippets are taken from https://github.com/karpathy/minGPT"
      ],
      "metadata": {
        "id": "uWPGKPjM-93N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word embedding\n",
        "### The following BPE code is adapted from paper:\n",
        "#### [Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/pdf/1508.07909.pdf)\n"
      ],
      "metadata": {
        "id": "j5poGeV3_EPj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pdb\n",
        "import string\n",
        "from collections import defaultdict\n",
        "\n",
        "#Byte pair encoding\n",
        "def word_to_charachter_tuple(word:str)->tuple:\n",
        "    '''\n",
        "    Converts a word into a tuple of characters along with an end character.\n",
        "    \"word\" -> ('w','o','r','d','</w>')\n",
        "    '''\n",
        "    word.strip()\n",
        "    word = \"\".join(ch for ch in word if ch.isalnum()) # keep only alpha-numeric characters\n",
        "    _lst = list(word.lower())\n",
        "    _lst.append(\"<\\w>\") # add end of word\n",
        "    return tuple(_lst)\n",
        "\n",
        "def get_pairs(word_as_tuple:tuple)->list:\n",
        "    '''\n",
        "    returns ('w','o','r','d','</w>') -> [('w','o'),('o,'r'),('r','d'),('d','</w>')]\n",
        "    '''\n",
        "    output = []\n",
        "    for i in range(len(word_as_tuple)-1):\n",
        "        output.append((word_as_tuple[i],word_as_tuple[i+1]))\n",
        "    return output\n",
        "\n",
        "def replace_pair(word_as_tuple:tuple, pair:tuple)->tuple:\n",
        "    '''\n",
        "    Given word = ('w','o','r','d','</w>') and pair = ('o,'r')\n",
        "    returns ('w','or','d','</w>'). Replacement happens for each occurance of the pair\n",
        "    '''\n",
        "    word = word_as_tuple\n",
        "    new_word = list()\n",
        "    is_last_char_used = False\n",
        "    i=0\n",
        "    while i < len(word)-1:\n",
        "        if (word[i],word[i+1]) == pair:\n",
        "            new_word.append( word[i]+word[i+1] )\n",
        "            if i == len(word)-2: is_last_char_used = True\n",
        "            i += 1 # skip the next merged character\n",
        "        else:\n",
        "            new_word.append(word[i])\n",
        "\n",
        "        i += 1\n",
        "\n",
        "    if not is_last_char_used : new_word.append(word[len(word)-1])\n",
        "\n",
        "    return tuple(new_word)\n",
        "\n",
        "def construct_word_vocab(word_vocab:defaultdict, file_path:str)->dict:\n",
        "    '''\n",
        "    Read file and update word_vocab dict. word_vocab has format word_vocab[('w','o','r','d')]->freq\n",
        "    '''\n",
        "    with open(file_path) as fp:\n",
        "        for line in fp:\n",
        "            for item in line.split(): #split ignore multiple spaces\n",
        "                item_as_tuple = word_to_charachter_tuple(item) # tuple to make a hashable dict key\n",
        "                if len(item_as_tuple) == 0: continue #ignore empty key\n",
        "                word_vocab[item_as_tuple] += 1\n",
        "\n",
        "    return word_vocab\n",
        "\n",
        "def get_byte_pair_hist(word_vocab:defaultdict)->dict:\n",
        "    '''\n",
        "    Read word_vocab[('w','o','r','d','\\w')]->freq and construct byte pair histogram\n",
        "    returns pair[('w','o')]->freq\n",
        "    '''\n",
        "    pair = defaultdict(int)\n",
        "    for word, freq in word_vocab.items():\n",
        "        for bigram in get_pairs(word):\n",
        "            pair[bigram] += freq\n",
        "\n",
        "    return pair\n",
        "\n",
        "def merge_pair(pair:tuple, word_vocab_in:dict)->dict:\n",
        "    '''\n",
        "    merge the input pair in the key of the word_vocab dict. E.g. pair=('w','o') then update\n",
        "    word_vocab_in[('w','o','r','d')]->freq to\n",
        "    word_vocab_in[('wo','r','d')]->freq\n",
        "    '''\n",
        "    word_vocab_out = {}\n",
        "    for word, freq in word_vocab_in.items():\n",
        "        new_word = replace_pair(word, pair)\n",
        "        word_vocab_out[new_word] = freq\n",
        "    return word_vocab_out\n",
        "\n",
        "def byte_pair_encoding(word_vocab:defaultdict, n:int=10)->dict:\n",
        "    '''\n",
        "    Given word_vocab[('w','o','r','d')]->freq merges pairs with highest frequency 'n' times.\n",
        "    E.g. a merge involves replacing ('w','o'), bigram with highest freq., as word_vocab_in[('wo','r','d')]->freq.\n",
        "    returns:\n",
        "    Merge rank bpe_ranks[ ('w','o') ]-> 0 (implies first merge),\n",
        "    Merged word vocab - word_vocab[('wo','r','d')]->freq\n",
        "    '''\n",
        "    i = 0\n",
        "    merges = []\n",
        "    for i in range(n):\n",
        "        pairs = get_byte_pair_hist(word_vocab)\n",
        "        best = max(pairs, key=pairs.get)\n",
        "        word_vocab = merge_pair(best, word_vocab)\n",
        "        merges.append(best)\n",
        "\n",
        "    # bpe merge list that defines the bpe \"tree\", of tuples (a,b) that are to merge to token ab\n",
        "    bpe_ranks = dict(zip(merges, range(len(merges))))\n",
        "    return bpe_ranks, word_vocab\n",
        "\n",
        "def get_bpe_encoder_decoder_map(word_vocab:defaultdict)->tuple:\n",
        "    '''\n",
        "    Given word_vocab[('wo','r','d')]->freq of merged words\n",
        "    returns:\n",
        "    Encoder encoder['wo')] -> id\n",
        "    Decoder decoder[ id ] -> 'wo'\n",
        "    '''\n",
        "    # assign an id for each token\n",
        "    bpe_encoder = {}; bpe_decoder = {}; id = 0\n",
        "    for key, value in word_vocab.items():\n",
        "        for token in key:\n",
        "            if token not in bpe_encoder:\n",
        "                bpe_encoder[token] = id\n",
        "                bpe_decoder[id] = token\n",
        "                id+=1\n",
        "\n",
        "    return bpe_encoder, bpe_decoder\n",
        "\n"
      ],
      "metadata": {
        "id": "0Ug-MKi2_G9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get data file\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "!mv input.txt sample_data/"
      ],
      "metadata": {
        "id": "xENHd64F_DLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"sample_data/input.txt\""
      ],
      "metadata": {
        "id": "YNfY72IKF9V6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing:\n",
        "word_vocab = defaultdict(int)\n",
        "word_vocab = construct_word_vocab(word_vocab, file_path)\n",
        "print(\"is empty string a key in word_vocab:\",() in word_vocab)\n",
        "pairs = get_byte_pair_hist(word_vocab)\n",
        "best = max(pairs, key=pairs.get)\n",
        "word_vocab_out = merge_pair(('f','i'), word_vocab)\n",
        "\n",
        "print(\"word vocab -> \",word_vocab)\n",
        "print(\"byte pair hist -> \",pairs)\n",
        "print(\"best pair -> \",best)\n",
        "print(\"merge f,i-> \",word_vocab_out)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KVwl4FhpowqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize word_vocab\n",
        "initial_vocab = list(string.ascii_lowercase)\n",
        "initial_vocab.extend(list(string.digits))\n",
        "initial_vocab = [(str(val)) for val in initial_vocab] # convert each key to a tuple\n",
        "word_vocab = defaultdict(int,zip(initial_vocab, [1]*len(initial_vocab)))\n",
        "\n",
        "#construct from a file\n",
        "word_vocab = construct_word_vocab(word_vocab, file_path)\n",
        "merge_ranks, word_vocab = byte_pair_encoding(word_vocab, 200)\n",
        "bpe_encoder, bpe_decoder = get_bpe_encoder_decoder_map(word_vocab)\n",
        "\n",
        "print(\"number of unique words: \",len(word_vocab))\n",
        "print(\"number of tokens: \", len(bpe_encoder))\n",
        "print(\"bpe encoder map-> \", bpe_encoder)\n",
        "print(\"bpe decoder map-> \", bpe_decoder)\n",
        "print(\"merges -> \", merge_ranks)\n"
      ],
      "metadata": {
        "id": "5rwLM-5kr6kx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cache={}\n",
        "def bpe_tokenize(input:str, merge_ranks:dict, bpe_encoder:dict)->list:\n",
        "    tokens = []\n",
        "    for word in input.split():\n",
        "        if word in cache: return cache[word]\n",
        "\n",
        "        word_tuple = word_to_charachter_tuple(word)\n",
        "        while True:\n",
        "            if len(word_tuple) == 1: break #Cannot get pair from a single element\n",
        "            pairs = get_pairs(word_tuple)\n",
        "            bigram = min(pairs, key = lambda pair: merge_ranks.get(pair, float('inf'))) # find the next lowest rank bigram that can be merged\n",
        "            if bigram not in merge_ranks: break # no more bigrams are eligible to be merged\n",
        "            word_tuple = replace_pair(word_tuple, bigram)\n",
        "\n",
        "        for token in word_tuple:\n",
        "            if token in bpe_encoder: tokens.append( bpe_encoder[token] )\n",
        "            else: raise Exception(\"unknown token: \"+ token)\n",
        "\n",
        "    return tokens\n"
      ],
      "metadata": {
        "id": "oFQPV81C16H9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test\n",
        "line = \"I am tokenizing\"\n",
        "tokens = bpe_tokenize(line, merge_ranks, bpe_encoder)\n",
        "print(\"Tokens ids -> \",tokens)\n",
        "print(\"Tokens -> \",[bpe_decoder[key] for key in tokens])\n",
        "reconstruction = [bpe_decoder[key] for key in tokens]\n",
        "reconstruction = \"\".join(reconstruction)\n",
        "print(\"Reconstruction-> \",reconstruction.replace('<\\w>', ' '))"
      ],
      "metadata": {
        "id": "eWT89t7P1ZxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Positional Encoding\n",
        "##### There are many position encoding methods. We will discuss\n",
        "\n",
        "\n",
        "*   Sinusoidal Position Encoding ([Explanation](https://timodenk.com/blog/linear-relationships-in-the-transformers-positional-encoding/))\n",
        "*   Relative Position Encoding ([Visualization](https://www.lesswrong.com/posts/qvWP3aBDBaqXvPNhS/gpt-2-s-positional-embedding-matrix-is-a-helix))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tN8q7CY3_Hp-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Sinusoidal position encoding\n",
        "import torch\n",
        "\n",
        "def positional_embedding(n_pos:int = 32, d_model:int = 64)->torch.tensor:\n",
        "    position = torch.arange(n_pos).unsqueeze(1)\n",
        "    i = torch.arange(d_model).unsqueeze(0)\n",
        "    deno = 1/ torch.pow(10000, (2 * (i // 2) / d_model))\n",
        "    pos_mat = position*deno\n",
        "    pos_mat[:,0::2] = torch.sin(pos_mat[:,0::2])\n",
        "    pos_mat[:,1::2] = torch.sin(pos_mat[:,0::2])\n",
        "    return pos_mat\n",
        "\n",
        "print(positional_embedding().size())\n"
      ],
      "metadata": {
        "id": "RPuQMr6H_KAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer"
      ],
      "metadata": {
        "id": "0wE7fcxq_K0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Code taken from https://github.com/karpathy/minGPT/blob/master/mingpt/model.py\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "class GELU(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n",
        "    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n",
        "    \"\"\"\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n",
        "\n",
        "class Causal_Self_Attention(nn.Module):\n",
        "    \"\"\"\n",
        "    A vanilla multi-head masked self-attention layer with a projection at the end.\n",
        "    It is possible to use torch.nn.MultiheadAttention here but I am including an\n",
        "    explicit implementation here to show that there is nothing too scary here.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        # regularization\n",
        "        self.attn_dropout = nn.Dropout(config.attn_pdrop)\n",
        "        self.resid_dropout = nn.Dropout(config.resid_pdrop)\n",
        "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                                     .view(1, 1, config.block_size, config.block_size))\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        att = self.attn_dropout(att)\n",
        "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "\n",
        "        # output projection\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "\n",
        "class Transformer_Block(nn.Module):\n",
        "    \"\"\" an unassuming Transformer block \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = Causal_Self_Attention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = nn.ModuleDict(dict(\n",
        "            c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd),\n",
        "            c_proj  = nn.Linear(4 * config.n_embd, config.n_embd),\n",
        "            act     = GELU(),\n",
        "            dropout = nn.Dropout(config.resid_pdrop),\n",
        "        ))\n",
        "        m = self.mlp\n",
        "        self.mlpf = lambda x: m.dropout(m.c_proj(m.act(m.c_fc(x)))) # MLP forward\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlpf(self.ln_2(x))\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "wC57DsNd_R0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Network Setup"
      ],
      "metadata": {
        "id": "BhqAQ_-s_eKW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Config(object): pass\n",
        "\n",
        "config = Config()\n",
        "\n",
        "#Model parametes\n",
        "config.n_layer = 6\n",
        "config.n_head = 2\n",
        "config.n_embd =  32\n",
        "# these options must be filled in externally\n",
        "config.vocab_size = len(bpe_encoder)\n",
        "config.block_size = None\n",
        "# dropout hyperparameters\n",
        "config.embd_pdrop = 0.1\n",
        "config.resid_pdrop = 0.1\n",
        "config.attn_pdrop = 0.1\n",
        "\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.vocab_size is not None\n",
        "        assert config.block_size is not None\n",
        "        self.block_size = config.block_size\n",
        "\n",
        "        type_given = config.model_type is not None\n",
        "        params_given = all([config.n_layer is not None, config.n_head is not None, config.n_embd is not None])\n",
        "        assert type_given ^ params_given # exactly one of these (XOR)\n",
        "        if type_given:\n",
        "            # translate from model_type to detailed configuration\n",
        "            config.merge_from_dict({\n",
        "                # names follow the huggingface naming conventions\n",
        "                # GPT-1\n",
        "                'openai-gpt':   dict(n_layer=12, n_head=12, n_embd=768),  # 117M params\n",
        "                # GPT-2 configs\n",
        "                'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
        "                'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
        "                'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
        "                'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
        "                # Gophers\n",
        "                'gopher-44m':   dict(n_layer=8, n_head=16, n_embd=512),\n",
        "                # (there are a number more...)\n",
        "                # I made these tiny models up\n",
        "                'gpt-mini':     dict(n_layer=6, n_head=6, n_embd=192),\n",
        "                'gpt-micro':    dict(n_layer=4, n_head=4, n_embd=128),\n",
        "                'gpt-nano':     dict(n_layer=3, n_head=3, n_embd=48),\n",
        "            }[config.model_type])\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            drop = nn.Dropout(config.embd_pdrop),\n",
        "            h = nn.ModuleList([Transformer_Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = nn.LayerNorm(config.n_embd),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        # init all weights, and apply a special scaled init to the residual projections, per GPT-2 paper\n",
        "        self.apply(self._init_weights)\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith('c_proj.weight'):\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
        "\n",
        "        # report number of parameters (note we don't count the decoder parameters in lm_head)\n",
        "        n_params = sum(p.numel() for p in self.transformer.parameters())\n",
        "        print(\"number of parameters: %.2fM\" % (n_params/1e6,))\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            torch.nn.init.zeros_(module.bias)\n",
        "            torch.nn.init.ones_(module.weight)\n"
      ],
      "metadata": {
        "id": "ulrU1aFp_h6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Data"
      ],
      "metadata": {
        "id": "ZA2A2VJi_iea"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R4HP3YhA_mgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "0YvNcwEO_nLT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GkDncGaJ_o3O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}