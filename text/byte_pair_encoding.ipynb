{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMzn4p7VkD/sVdX/p/cYfs0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/blooming-ai/generativeai/blob/main/text/byte_pair_encoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Byte Pair Encoding\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uWPGKPjM-93N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### The following BPE code is adapted from the paper:\n",
        "#### [Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/pdf/1508.07909.pdf)\n",
        "#### Few code snippets are taken from https://github.com/karpathy/minGPT\n"
      ],
      "metadata": {
        "id": "j5poGeV3_EPj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pdb\n",
        "import string\n",
        "from collections import defaultdict\n",
        "\n",
        "#Byte pair encoding\n",
        "def word_to_charachter_tuple(word:str)->tuple:\n",
        "    '''\n",
        "    Converts a word into a tuple of characters along with an end character.\n",
        "    \"word\" -> ('w','o','r','d','</w>')\n",
        "    '''\n",
        "    word.strip()\n",
        "    word = \"\".join(ch for ch in word if ch.isalnum()) # keep only alpha-numeric characters\n",
        "    _lst = list(word.lower())\n",
        "    _lst.append(\"<\\w>\") # add end of word\n",
        "    return tuple(_lst)\n",
        "\n",
        "def get_pairs(word_as_tuple:tuple)->list:\n",
        "    '''\n",
        "    returns ('w','o','r','d','</w>') -> [('w','o'),('o,'r'),('r','d'),('d','</w>')]\n",
        "    '''\n",
        "    output = []\n",
        "    for i in range(len(word_as_tuple)-1):\n",
        "        output.append((word_as_tuple[i],word_as_tuple[i+1]))\n",
        "    return output\n",
        "\n",
        "def replace_pair(word_as_tuple:tuple, pair:tuple)->tuple:\n",
        "    '''\n",
        "    Given word = ('w','o','r','d','</w>') and pair = ('o,'r')\n",
        "    returns ('w','or','d','</w>'). Replacement happens for each occurance of the pair\n",
        "    '''\n",
        "    word = word_as_tuple\n",
        "    new_word = list()\n",
        "    is_last_char_used = False\n",
        "    i=0\n",
        "    while i < len(word)-1:\n",
        "        if (word[i],word[i+1]) == pair:\n",
        "            new_word.append( word[i]+word[i+1] )\n",
        "            if i == len(word)-2: is_last_char_used = True\n",
        "            i += 1 # skip the next merged character\n",
        "        else:\n",
        "            new_word.append(word[i])\n",
        "\n",
        "        i += 1\n",
        "\n",
        "    if not is_last_char_used : new_word.append(word[len(word)-1])\n",
        "\n",
        "    return tuple(new_word)\n",
        "\n",
        "def construct_word_vocab(word_vocab:defaultdict, file_path:str)->dict:\n",
        "    '''\n",
        "    Read file and update word_vocab dict. word_vocab has format word_vocab[('w','o','r','d')]->freq\n",
        "    '''\n",
        "    with open(file_path) as fp:\n",
        "        for line in fp:\n",
        "            for item in line.split(): #split ignore multiple spaces\n",
        "                item_as_tuple = word_to_charachter_tuple(item) # tuple to make a hashable dict key\n",
        "                if len(item_as_tuple) == 0: continue #ignore empty key\n",
        "                word_vocab[item_as_tuple] += 1\n",
        "\n",
        "    return word_vocab\n",
        "\n",
        "def get_byte_pair_hist(word_vocab:defaultdict)->dict:\n",
        "    '''\n",
        "    Read word_vocab[('w','o','r','d','\\w')]->freq and construct byte pair histogram\n",
        "    returns pair[('w','o')]->freq\n",
        "    '''\n",
        "    pair = defaultdict(int)\n",
        "    for word, freq in word_vocab.items():\n",
        "        for bigram in get_pairs(word):\n",
        "            pair[bigram] += freq\n",
        "\n",
        "    return pair\n",
        "\n",
        "def merge_pair(pair:tuple, word_vocab_in:dict)->dict:\n",
        "    '''\n",
        "    merge the input pair in the key of the word_vocab dict. E.g. pair=('w','o') then update\n",
        "    word_vocab_in[('w','o','r','d')]->freq to\n",
        "    word_vocab_in[('wo','r','d')]->freq\n",
        "    '''\n",
        "    word_vocab_out = {}\n",
        "    for word, freq in word_vocab_in.items():\n",
        "        new_word = replace_pair(word, pair)\n",
        "        word_vocab_out[new_word] = freq\n",
        "    return word_vocab_out\n",
        "\n",
        "def byte_pair_encoding(word_vocab:defaultdict, n:int=10)->dict:\n",
        "    '''\n",
        "    Given word_vocab[('w','o','r','d')]->freq merges pairs with highest frequency 'n' times.\n",
        "    E.g. a merge involves replacing ('w','o'), bigram with highest freq., as word_vocab_in[('wo','r','d')]->freq.\n",
        "    returns:\n",
        "    Merge rank bpe_ranks[ ('w','o') ]-> 0 (implies first merge),\n",
        "    Merged word vocab - word_vocab[('wo','r','d')]->freq\n",
        "    '''\n",
        "    i = 0\n",
        "    merges = []\n",
        "    for i in range(n):\n",
        "        pairs = get_byte_pair_hist(word_vocab)\n",
        "        best = max(pairs, key=pairs.get)\n",
        "        word_vocab = merge_pair(best, word_vocab)\n",
        "        merges.append(best)\n",
        "\n",
        "    # bpe merge list that defines the bpe \"tree\", of tuples (a,b) that are to merge to token ab\n",
        "    bpe_ranks = dict(zip(merges, range(len(merges))))\n",
        "    return bpe_ranks, word_vocab\n",
        "\n",
        "def get_bpe_encoder_decoder_map(word_vocab:defaultdict)->tuple:\n",
        "    '''\n",
        "    Given word_vocab[('wo','r','d')]->freq of merged words\n",
        "    returns:\n",
        "    Encoder encoder['wo')] -> id\n",
        "    Decoder decoder[ id ] -> 'wo'\n",
        "    '''\n",
        "    # assign an id for each token\n",
        "    bpe_encoder = {}; bpe_decoder = {}; id = 0\n",
        "    for key, value in word_vocab.items():\n",
        "        for token in key:\n",
        "            if token not in bpe_encoder:\n",
        "                bpe_encoder[token] = id\n",
        "                bpe_decoder[id] = token\n",
        "                id+=1\n",
        "\n",
        "    return bpe_encoder, bpe_decoder\n",
        "\n"
      ],
      "metadata": {
        "id": "0Ug-MKi2_G9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get data file\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "!mv input.txt sample_data/"
      ],
      "metadata": {
        "id": "xENHd64F_DLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"sample_data/input.txt\""
      ],
      "metadata": {
        "id": "YNfY72IKF9V6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing:\n",
        "word_vocab = defaultdict(int)\n",
        "word_vocab = construct_word_vocab(word_vocab, file_path)\n",
        "print(\"is empty string a key in word_vocab:\",() in word_vocab)\n",
        "pairs = get_byte_pair_hist(word_vocab)\n",
        "best = max(pairs, key=pairs.get)\n",
        "word_vocab_out = merge_pair(('f','i'), word_vocab)\n",
        "\n",
        "print(\"word vocab -> \",word_vocab)\n",
        "print(\"byte pair hist -> \",pairs)\n",
        "print(\"best pair -> \",best)\n",
        "print(\"merge f,i-> \",word_vocab_out)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KVwl4FhpowqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize word_vocab\n",
        "initial_vocab = list(string.ascii_lowercase)\n",
        "initial_vocab.extend(list(string.digits))\n",
        "initial_vocab = [(str(val)) for val in initial_vocab] # convert each key to a tuple\n",
        "word_vocab = defaultdict(int,zip(initial_vocab, [1]*len(initial_vocab)))\n",
        "\n",
        "#construct from a file\n",
        "word_vocab = construct_word_vocab(word_vocab, file_path)\n",
        "merge_ranks, word_vocab = byte_pair_encoding(word_vocab, 200)\n",
        "bpe_encoder, bpe_decoder = get_bpe_encoder_decoder_map(word_vocab)\n",
        "\n",
        "print(\"number of unique words: \",len(word_vocab))\n",
        "print(\"number of tokens: \", len(bpe_encoder))\n",
        "print(\"bpe encoder map-> \", bpe_encoder)\n",
        "print(\"bpe decoder map-> \", bpe_decoder)\n",
        "print(\"merges -> \", merge_ranks)\n"
      ],
      "metadata": {
        "id": "5rwLM-5kr6kx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cache={}\n",
        "def bpe_tokenize(input:str, merge_ranks:dict, bpe_encoder:dict)->list:\n",
        "    tokens = []\n",
        "    for word in input.split():\n",
        "        if word in cache: return cache[word]\n",
        "\n",
        "        word_tuple = word_to_charachter_tuple(word)\n",
        "        while True:\n",
        "            if len(word_tuple) == 1: break #Cannot get pair from a single element\n",
        "            pairs = get_pairs(word_tuple)\n",
        "            bigram = min(pairs, key = lambda pair: merge_ranks.get(pair, float('inf'))) # find the next lowest rank bigram that can be merged\n",
        "            if bigram not in merge_ranks: break # no more bigrams are eligible to be merged\n",
        "            word_tuple = replace_pair(word_tuple, bigram)\n",
        "\n",
        "        for token in word_tuple:\n",
        "            if token in bpe_encoder: tokens.append( bpe_encoder[token] )\n",
        "            else: raise Exception(\"unknown token: \"+ token)\n",
        "\n",
        "    return tokens\n"
      ],
      "metadata": {
        "id": "oFQPV81C16H9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test\n",
        "line = \"I am tokenizing\"\n",
        "tokens = bpe_tokenize(line, merge_ranks, bpe_encoder)\n",
        "print(\"Tokens ids -> \",tokens)\n",
        "print(\"Tokens -> \",[bpe_decoder[key] for key in tokens])\n",
        "reconstruction = [bpe_decoder[key] for key in tokens]\n",
        "reconstruction = \"\".join(reconstruction)\n",
        "print(\"Reconstruction-> \",reconstruction.replace('<\\w>', ' '))"
      ],
      "metadata": {
        "id": "eWT89t7P1ZxH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}