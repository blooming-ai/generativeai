{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMJTd+X1A2ZG4HQvLwHh3sD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/blooming-ai/generativeai/blob/main/generativeai/examples/generative/ipynb/gpt_basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT code\n",
        "https://github.com/karpathy/minGPT"
      ],
      "metadata": {
        "id": "uWPGKPjM-93N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get data file\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xENHd64F_DLN",
        "outputId": "5e250555-5106-4a24-c867-bf3d57b0f588"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-07-31 08:36:16--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2023-07-31 08:36:16 (19.6 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word embedding"
      ],
      "metadata": {
        "id": "j5poGeV3_EPj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"sample_data/input.txt\""
      ],
      "metadata": {
        "id": "YNfY72IKF9V6"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pdb\n",
        "import string\n",
        "#Inspired from https://www.geeksforgeeks.org/byte-pair-encoding-bpe-in-nlp/\n",
        "# But rewritten for better understanding and working on files\n",
        "\n",
        "#Byte pair encoding\n",
        "def construct_word_vocab(word_vocab:dict, file_path:str)->dict:\n",
        "    '''\n",
        "    Read file and update word_vocab dict. word_vocab has format word_vocab[('w','o','r','d')]->freq\n",
        "    '''\n",
        "    with open(file_path) as fp:\n",
        "        while True:\n",
        "            line = fp.readline()\n",
        "            if not line: break\n",
        "            else:\n",
        "                lst = line.split()\n",
        "                for item in lst:\n",
        "                    item.strip()\n",
        "                    item = \"\".join(ch for ch in item if ch.isalnum()) # keep only alpha-numeric characters\n",
        "                    lst_item = tuple(item.lower()) # tuple to make a hashable dict key\n",
        "                    if len(lst_item) == 0: continue #ignore empty key\n",
        "                    if lst_item not in word_vocab: word_vocab[lst_item] = 1\n",
        "                    else:                     word_vocab[lst_item] += 1\n",
        "\n",
        "    return word_vocab\n",
        "\n",
        "def get_byte_pair_hist(word_vocab:dict)->dict:\n",
        "    '''\n",
        "    Read word_vocab[('w','o','r','d')]->freq and construct byte pair histogram\n",
        "    returns pair[('w','o')]->freq\n",
        "    '''\n",
        "    pair = {}\n",
        "    for word, freq in word_vocab.items():\n",
        "        for i in range(len(word)-1):\n",
        "            key = (word[i],word[i+1])\n",
        "            if key not in pair: pair[key] = freq\n",
        "            else:               pair[key] += freq\n",
        "\n",
        "    return pair\n",
        "\n",
        "def merge_pair(replace_pair:tuple, word_vocab_in:dict)->dict:\n",
        "    '''\n",
        "    replace pair in the key of the word_vocab. E.g. pair=('w','o') then update\n",
        "    word_vocab_in[('w','o','r','d')]->freq to\n",
        "    word_vocab_in[('wo','r','d')]->freq\n",
        "    '''\n",
        "    word_vocab_out = {}\n",
        "    for word, freq in word_vocab_in.items():\n",
        "        word_new = list()\n",
        "        is_last_char_used = False\n",
        "        i=0\n",
        "        while i < len(word)-1:\n",
        "            if (word[i],word[i+1]) == replace_pair:\n",
        "                word_new.append(word[i]+word[i+1])\n",
        "                if i == len(word)-2: is_last_char_used = True\n",
        "                i += 1 # skip the next merged character\n",
        "            else:\n",
        "                word_new.append(word[i])\n",
        "\n",
        "            i += 1\n",
        "\n",
        "        if not is_last_char_used : word_new.append(word[len(word)-1])\n",
        "        word_vocab_out[tuple(word_new)] = freq\n",
        "\n",
        "    return word_vocab_out\n",
        "\n",
        "def byte_pair_encoding(word_vocab:dict, n:int=0)->dict:\n",
        "    '''\n",
        "    Given word_vocab_in[('w','o','r','d')]->freq merges pairs with highest frequency 'n'.\n",
        "    If n==0, runs till all pairs are merged.\n",
        "    E.g. if ('w','o') has highest freq. then word_vocab_in[('wo','r','d')]->freq.\n",
        "    Also returns the final vocab.\n",
        "    '''\n",
        "    i = 0\n",
        "    vocab = list(string.ascii_lowercase)\n",
        "    vocab.extend(list(string.digits))\n",
        "    # print(vocab)\n",
        "    while True:\n",
        "        pairs = get_byte_pair_hist(word_vocab)\n",
        "        best = max(pairs, key=pairs.get)\n",
        "        vocab.append(best[0]+best[1])\n",
        "        if pairs[best] == 1: break # do not replace if pair occurs only once.\n",
        "        word_vocab = merge_pair(best, word_vocab)\n",
        "        if(i>= n and n>0): break # break if i exceeds n\n",
        "        i += 1\n",
        "\n",
        "    return word_vocab, vocab\n"
      ],
      "metadata": {
        "id": "0Ug-MKi2_G9J"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing:\n",
        "# word_vocab = {}\n",
        "# word_vocab = construct_word_vocab(word_vocab, file_path)\n",
        "# print(\"is empyt string a key in word_vocab:\",() in word_vocab)\n",
        "# pairs = get_byte_pair_hist(word_vocab)\n",
        "# best = max(pairs, key=pairs.get)\n",
        "# word_vocab_out = merge_pair(('f','i'), word_vocab)\n",
        "\n",
        "# print(word_vocab)\n",
        "# print(pairs)\n",
        "# print(best)\n",
        "# print(word_vocab_out)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KVwl4FhpowqH"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_vocab = construct_word_vocab(dict(), file_path)\n",
        "bpe_word_vocab, vocab = byte_pair_encoding(word_vocab, 200)\n",
        "print(vocab)"
      ],
      "metadata": {
        "id": "5rwLM-5kr6kx",
        "outputId": "179c675f-86fb-401e-d345-c1aa199cf289",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'th', 'ou', 'an', 'er', 'in', 'the', 'or', 'en', 'ar', 'is', 'es', 'on', 'and', 'at', 'll', 'to', 'st', 'you', 'me', 'no', 'ha', 'ing', 'se', 'of', 'wh', 'le', 'wi', 'be', 'he', 're', 'it', 've', 'ch', 'ro', 'my', 'for', 'as', 'ce', 'ay', 'that', 'ed', 'li', 'ir', 'ld', 'we', 'ut', 'ere', 'ke', 'not', 'us', 'ri', 'de', 'lo', 'with', 'so', 'gh', 'ent', 'co', 'thou', 'your', 'go', 'hi', 'ow', 'our', 'et', 'al', 'ad', 'ther', 'his', 'but', 'un', 'this', 'io', 'all', 'est', 'have', 'ard', 'ly', 'ur', 'do', 'ght', 'ra', 'him', 'ma', 'king', 'od', 'ord', 'ess', 'what', 'now', 'am', 'pe', 'thy', 'ver', 'ill', 'sha', 'are', 'fe', 'id', 'will', 'her', 'ould', 'fa', 'ck', 'ge', 'one', 'man', 'ne', 'by', 'ru', 'su', 'la', 'ta', 'pr', 'ti', 'if', 'po', 'con', 'ter', 'sh', 'shall', 'lord', 'end', 'qu', 'mor', 'thee', 'whi', 'com', 'art', 'come', 'up', 'bo', 'ca', 'ry', 'ho', 'let', 'good', 'well', 'mo', 'ain', 'fro', 'ence', 'du', 'oun', 'ous', 'sel', 'mar', 'si', 'mu', 'she', 'sir', 'te', 'pro', 'then', 'per', 'from', 'ius', 'was', 'than', 'ine', 'ath', 'there', 'who', 'more', 'ant', 'ion', 'which', 'ul', 'ble', 'here', 'would', 'ven', 'row', 'can', 'they', 'ward', 'ind', 'men', 'ate', 'say', 'ong', 'how', 'out', 'ight', 'sa', 'wor', 'them', 'know', 'bro', 'self', 'ak', 'see', 'ers', 'ct', 'their', 'rich', 'love', 'gre', 'wo', 'ive', 'il']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Positional Encoding"
      ],
      "metadata": {
        "id": "tN8q7CY3_Hp-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Sinusoidal position encoding"
      ],
      "metadata": {
        "id": "RPuQMr6H_KAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer"
      ],
      "metadata": {
        "id": "0wE7fcxq_K0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "def def_value():\n",
        "  '''\n",
        "  Function to return a default values for a key that is not present in\n",
        "  defaultdict\n",
        "  '''\n",
        "  return 0\n",
        "a = 'aa'\n",
        "di = defaultdict()\n",
        "di[a] += 1\n",
        "print(di)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "wC57DsNd_R0c",
        "outputId": "8265b191-0b6c-4a4d-c77d-3410abce6fcd"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-415c96786a08>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'aa'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'aa'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Network Setup"
      ],
      "metadata": {
        "id": "BhqAQ_-s_eKW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ulrU1aFp_h6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Data"
      ],
      "metadata": {
        "id": "ZA2A2VJi_iea"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R4HP3YhA_mgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "0YvNcwEO_nLT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GkDncGaJ_o3O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}